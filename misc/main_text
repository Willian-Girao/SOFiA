
introduction

Metaheuristics optimization algorithms have become very popular and have been successfully applied to solve many optimization problems in an 
increasingly wider range of fields, specially in engineering and Artificial Intelligence. In such fields, various problems have related noisy, multimodal, and non-linear complex functions which are most often non-differentiable and therefore can not be efficiently optimized by conventional numerical methods, like the ones based on gradient descent, hence the need for more flexible methods such as metaheuristics. There are basically three main reasons for their widely usage: simplicity, flexibility, and a favorable capability to handle the pitfalls of local optima.

The overall majority of metaheuristics are modeled after conceptually simple mechanisms. The aspects of interest related to these mechanisms observed in the natural world are either at the biological level, such as fundamentaly basic concepts of genetics like the ones used in Genetic Algorithms (GAs)\cite{Ref:01}, or at the interspecies and intraspecies interaction level, such as the movement of organisms in a bird flock or fish school modeled in the Particle Swarm Optimization (PSO)\cite{kennedy:1995:PartSwarmOpt} or the pack hierarchy and hunting habits of gray wolves in the Gray Wolf Optimizer (GWO)\cite{MIRJALILI201446}. Much of the simplicity of the metaheuristics come from a simplified modeling of such mechanisms, allowing scientist to quickly prototype algorithms and validate the underlying assumptions made to build the model.

The flexibility comes from the nature of metaheuristics themselves: it assumes problems as black boxes. That is, no a priori knowledge of the problem is required to solve it. This property is also what makes it possible to use such methods in a variety of problems without making any profound changes to the core of the metaheuristic.

Finally, giving its stochastic nature, these methods are less susceptible to stagnation at local optima, specially when they work with populations of solutions. Usually, the search spaces of real world problems are unknown, virtually infinity, very complex, and most likely present a massive number of local optimal values, which makes it unfeasible to perform a thoroughly search. Therefore, the guided random behaviour of the navigation through complex search spaces is what makes metaheuristics suitable to tackle problems with this characteristics.

Metaheuristics can be roughly devided into two categories: population-based and single-solution-based. In the population-based ones, an initial population of $n$ candidate solutions is generated (either randomly or guided by an heuristic), which is improved throughout the process by following some rules of interactions between candidate solutions (PSO and GWO are examples of thi categorie). In the single-solution-based approaches (like Simulated Annealing \cite{Kirkpatrick1983OptimizationBS}), a single initial solution is improved over the course of iteractions.

One of the most notable branches of population-based metaheuristics is the Swarm Intelligence (SI).
%% talk about swarm intelligence here %%

Independetly of the categorie of the method, all metaheuristics have two basic main steps: exploration and exploitation. In the exploration phase, the method tries to explore the search space of the problem as broadly as possible in order to detect possible areas of intereste. In the exploitation phase, usually refered to as local search, the method performs an in depth search of the area selected in the later phase in order to better explore it. The ability to decide what areas to explore in a more detailed manner is a difficult task and it is directly influenced by the way the algorithm copes with its stochastic nature. This work proposes a new SI algorithm based on the social aspects of oppinion formation designed to enhance the commom exploration phase in order to cope with a large viriaty of search spaces with different properties while maintaining high performance.
